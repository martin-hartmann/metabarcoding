# create a project directory that includes the empty subdirectories "1_raw", "2_processed"
# copy this script into the project directory
# copy the raw sequencing files "*_R1.fastq.gz" and "*_R1.fastq.gz" into the "1_raw" directory
# edit information regarding primers and databases
# move with your command line terminal to the project directory and start the script with "bash scriptname.bash"
# this script assumes all databases have been built

# PRIMERS
primerF="CCTAYGGGDBGCWSCAG" # Frey et al. 2016 (https://doi.org/10.1093/femsec/fiw018)
primerR="GGACTACNVGGGTHTCTAAT" # Frey et al. 2016 (https://doi.org/10.1093/femsec/fiw018)

# DATABASES
## the phiX genome sequences can be retrieved from https://www.ncbi.nlm.nih.gov/nuccore/NC_001422.1
## the SILVA database can be downloaded from https://www.arb-silva.de/download/archive/ and primers specific databases are generated by the following command
### cutadapt -g "$primerF;required;e=0;o=${#primerF}...$primerR;required;e=0;o=${#primerR}" --max-n=0 --trimmed-only -o out.fasta in.fasta
PHIX="/databases/phix"
TAX="/databases/silva_v132NR.341F806R.sintax" # rename to your file

# CPUs
T="80"


# PREPARE
cd 1_raw
parallel -j $T 'gunzip -k {}' ::: *gz # unpack
# xargs -a rename.txt -n 2 mv # rename files based on 2-column find-replace list saved in rename.txt (optional)
parallel -j $T "awk '{if (NR%4==1) {sub(\"_R1.fastq\",\"\",FILENAME); sub(\"_R2.fastq\",\"\",FILENAME); print \$1 \";sample=\" FILENAME \";\"} else {print \$0}}' {} > {}.tmp; mv {}.tmp {}" ::: *fastq # add filename to sequence header
cat *R1.fastq > ../2_processed/1.all.raw.R1.fastq # concatenate R1
cat *R2.fastq > ../2_processed/1.all.raw.R2.fastq # concatenate R2
rm *fastq # delete single files
cd ../2_processed # move to new directory

# QC
fastqc -t $T 1.all.raw.R1.fastq # QC on R1
fastqc -t $T 1.all.raw.R2.fastq # QC on R2
vsearch --fastq_eestats 1.all.raw.R1.fastq --output 1.all.raw.R1.eestats.txt # quality stats R1
vsearch --fastq_eestats 1.all.raw.R2.fastq --output 1.all.raw.R2.eestats.txt # quality stats R2

# QUALITY FILTER AND ASV/OTU DELINEATION
bowtie2 -x $PHIX -1 1.all.raw.R1.fastq -2 1.all.raw.R2.fastq --un-conc all.nophix --fast -p $T > /dev/null # remove phix
mv all.1.nophix 2.all.nophix.R1.fastq & mv all.2.nophix 2.all.nophix.R2.fastq # rename
cutadapt -j $T -g ^$primerF -G ^$primerR -e 0.12 -m 1 --trimmed-only -o 3.all.trim.R1.fastq -p 3.all.trim.R2.fastq 2.all.nophix.R1.fastq 2.all.nophix.R2.fastq # trim primers
vsearch --fastq_mergepairs 3.all.trim.R1.fastq --reverse 3.all.trim.R2.fastq --fastqout 4.all.merge.notrim.fastq --fastaout 4.all.merge.notrim.fasta --fastq_truncqual 7 --fastq_allowmergestagger --fastq_minovlen 30 --threads $T # merge PE reads
vsearch --fastq_mergepairs 3.all.trim.R1.fastq --reverse 3.all.trim.R2.fastq --fastqout 4.all.merge.fastq --fastaout 4.all.merge.fasta --fastq_truncqual 7 --fastq_allowmergestagger --fastq_minovlen 30 --fastq_minmergelen 300 --threads $T # merge PE reads
vsearch --fastq_eestats 4.all.merge.notrim.fastq --output 4.all.merge.notrim.eestats.txt # quality stats for merged reads
vsearch --fastq_eestats 4.all.merge.fastq --output 4.all.merge.eestats.txt # quality stats for merged reads
vsearch --fastq_filter 4.all.merge.fastq --fastaout 5.all.eefilter.fasta --fastq_maxee 1 --threads $T # merge and filter
vsearch --derep_fulllength 5.all.eefilter.fasta --sizeout --relabel Uniq --output 6.all.uniq.fasta # dereplicate sequences
vsearch --cluster_unoise 6.all.uniq.fasta --centroids 6.all.ASV.fasta --uc 6.all.ASV.uc --relabel ASV --sizeorder --sizein --sizeout --minsize 8 --threads $T # delineate into ASVs
vsearch --uchime3_denovo 6.all.ASV.fasta --nonchimeras 7.all.ASV_nochim.fasta --uchimeout 7.all.ASV_nochim.txt --uchimealns 7.all.ASV_chimaln.txt --abskew 8 --sizein --sizeout # identify and remove chimeras
metaxa2_x -i 7.all.ASV_nochim.fasta -o 8.all.ASV_metaxa --allow_reorder F --graphical F --truncate F --complement F --cpu $T # verify target
awk -i inplace -F "|" '{print $1}' 8.all.ASV_metaxa.extraction.fasta # reformat metaxa file
seqkit sort -N -w 80 8.all.ASV_metaxa.extraction.fasta -o 8.all.ASV_metaxa.fasta # re-sort sequences by name
vsearch --cluster_size 8.all.ASV_metaxa.fasta --centroids 8.all.OTU_metaxa.fasta --uc 8.all.OTU_metaxa.uc --id 0.97 --relabel OTU --sizeorder --sizein --sizeout --threads $T # cluster into OTUs
awk -i inplace -F ";size" '{print $1}' 8.all.ASV_metaxa.fasta # reformat metaxa file
awk -i inplace -F ";size" '{print $1}' 8.all.OTU_metaxa.fasta # reformat metaxa file
vsearch --usearch_global 4.all.merge.fasta --db 8.all.ASV_metaxa.fasta --id 0.97 --maxhits 1 --maxaccepts 0 --uc 9.all.ASV_map.uc --matched 9.all.ASV_map.fasta --otutabout 9.all.ASV_map.txt --threads $T # map reads to ASVs
vsearch --usearch_global 4.all.merge.fasta --db 8.all.OTU_metaxa.fasta --id 0.97 --maxhits 1 --maxaccepts 0 --uc 9.all.OTU_map.uc --matched 9.all.OTU_map.fasta --otutabout 9.all.OTU_map.txt --threads $T # map reads to OTUs
sort -n -k1.4 -o 9.all.ASV_map.txt 9.all.ASV_map.txt # sort ASVs
sort -n -k1.4 -o 9.all.OTU_map.txt 9.all.OTU_map.txt # sort OTUs

# TAXONOMIC CLASSIFICATION
vsearch --sintax 8.all.ASV_metaxa.fasta --db $TAX --tabbedout 9.all.ASV_tax.sintax.txt --sintax_cutoff 0.8 --strand plus --threads $T # assign taxonomy with sintax
vsearch --sintax 8.all.OTU_metaxa.fasta --db $TAX --tabbedout 9.all.OTU_tax.sintax.txt --sintax_cutoff 0.8 --strand plus --threads $T # assign taxonomy with sintax
cat 9.all.ASV_tax.sintax.txt | sort -n -k1.4 | awk '{print $1 "\t" $4}' | sed 's/,/\t/g' | sed '1 i\ASV\tdomain\tphylum\tclass\torder\tfamily\tgenus' > 9.all.ASV_tax.sintax.rf.txt # reformat
cat 9.all.OTU_tax.sintax.txt | sort -n -k1.4 | awk '{print $1 "\t" $4}' | sed 's/,/\t/g' | sed '1 i\OTU\tdomain\tphylum\tclass\torder\tfamily\tgenus' > 9.all.OTU_tax.sintax.rf.txt # reformat
vsearch --usearch_global 8.all.ASV_metaxa.fasta --db $TAX --id 0.97 --maxaccepts 0 --maxrejects 0 --uc_allhits --uc 9.all.ASV_tax.lca.uc --lca_cutoff 0.9 --lcaout 9.all.ASV_tax.lca.txt --threads $T # assign taxonomy with LCA
vsearch --usearch_global 8.all.OTU_metaxa.fasta --db $TAX --id 0.97 --maxaccepts 0 --maxrejects 0 --uc_allhits --uc 9.all.OTU_tax.lca.uc --lca_cutoff 0.9 --lcaout 9.all.OTU_tax.lca.txt --threads $T # assign taxonomy with LCA
cat 9.all.ASV_tax.lca.txt | sort -n -k1.4 | sed 's/,/\t/g' | sed '1 i\ASV\tdomain\tphylum\tclass\torder\tfamily\tgenus\tspecies' > 9.all.ASV_tax.lca.rf.txt # reformat taxonomy
cat 9.all.OTU_tax.lca.txt | sort -n -k1.4 | sed 's/,/\t/g' | sed '1 i\OTU\tdomain\tphylum\tclass\torder\tfamily\tgenus\tspecies' > 9.all.OTU_tax.lca.rf.txt # reformat taxonomy
